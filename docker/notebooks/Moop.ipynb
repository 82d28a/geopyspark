{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rasterio.__gdal_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moop(a):\n",
    "    import rasterio\n",
    "    return rasterio.__gdal_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moop(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"moop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(moop).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "from pyspark.serializers import AutoBatchedSerializer\n",
    "from py4j.java_gateway import java_import\n",
    "from geopyspark.avroserializer import AvroSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"geopyspark.geotrellis.tests.schemas.ExtentWrapper\"\n",
    "java_import(sc._gateway.jvm, path)\n",
    "ew = sc._jvm.ExtentWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geopyspark.geopycontext import GeoPyContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tup = ew.testOut(sc._jsc.sc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "java_rdd = tup._1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ser = AvroSerializer(tup._2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = RDD(java_rdd, sc, AutoBatchedSerializer(ser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def moop1(a):\n",
    "    import os\n",
    "    return dict(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CLASSPATH': '/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002:/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002/__spark_conf__:/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002/__spark_libs__/*:/etc/hadoop/conf:/usr/lib/hadoop/*:/usr/lib/hadoop/lib/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-mapreduce/*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-yarn/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/lib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/lib/spark/yarn/lib/datanucleus-api-jdo.jar:/usr/lib/spark/yarn/lib/datanucleus-core.jar:/usr/lib/spark/yarn/lib/datanucleus-rdbms.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/lib/hadoop-mapreduce/share/hadoop/mapreduce/*:/usr/lib/hadoop-mapreduce/share/hadoop/mapreduce/lib/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/lib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*',\n",
       " 'CONF_DIR': '/etc/hadoop/conf',\n",
       " 'CONTAINER_ID': 'container_1490714960966_0007_01_000002',\n",
       " 'DAEMON': 'hadoop-yarn-nodemanager',\n",
       " 'DAEMON_FLAGS': 'nodemanager',\n",
       " 'DESC': 'Hadoop nodemanager',\n",
       " 'EXEC_PATH': '/usr/lib/hadoop-yarn/sbin/yarn-daemon.sh',\n",
       " 'HADOOP_CLASSPATH': ':/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/spark/yarn/lib/spark-yarn-shuffle.jar:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/spark/yarn/lib/spark-yarn-shuffle.jar',\n",
       " 'HADOOP_COMMON_HOME': '/usr/lib/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/etc/hadoop/conf',\n",
       " 'HADOOP_DATANODE_HEAPSIZE': '757',\n",
       " 'HADOOP_HDFS_HOME': '/usr/lib/hadoop-hdfs',\n",
       " 'HADOOP_HOME': '/usr/lib/hadoop',\n",
       " 'HADOOP_HOME_WARN_SUPPRESS': 'true',\n",
       " 'HADOOP_JOB_HISTORYSERVER_HEAPSIZE': '2396',\n",
       " 'HADOOP_LIBEXEC_DIR': '/usr/lib/hadoop/libexec',\n",
       " 'HADOOP_MAPRED_HOME': '/usr/lib/hadoop-mapreduce',\n",
       " 'HADOOP_NAMENODE_HEAPSIZE': '1740',\n",
       " 'HADOOP_OPTS': \" -server -XX:OnOutOfMemoryError='kill -9 %p' -Dhadoop.log.dir=/usr/lib/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/lib/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -XX:OnOutOfMemoryError='kill -9 %p' -Dhadoop.log.dir=/usr/lib/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/lib/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true\",\n",
       " 'HADOOP_PREFIX': '/usr/lib/hadoop',\n",
       " 'HADOOP_TOKEN_FILE_LOCATION': '/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002/container_tokens',\n",
       " 'HADOOP_YARN_HOME': '/usr/lib/hadoop-yarn',\n",
       " 'HOME': '/home/',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-openjdk',\n",
       " 'JAVA_LIBRARY_PATH': ':/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native',\n",
       " 'JSVC_HOME': '/usr/lib/bigtop-utils',\n",
       " 'JVM_PID': '10295',\n",
       " 'LANG': 'en_US.UTF-8',\n",
       " 'LD_LIBRARY_PATH': 'gdal-and-friends.tar.gz/lib',\n",
       " 'LOCAL_DIRS': '/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007,/mnt1/yarn/usercache/hadoop/appcache/application_1490714960966_0007',\n",
       " 'LOGNAME': 'hadoop',\n",
       " 'LOG_DIRS': '/var/log/hadoop-yarn/containers/application_1490714960966_0007/container_1490714960966_0007_01_000002',\n",
       " 'MALLOC_ARENA_MAX': '4',\n",
       " 'NLSPATH': '/usr/dt/lib/nls/msg/%L/%N.cat',\n",
       " 'NM_AUX_SERVICE_mapreduce_shuffle': 'AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\\r\\n',\n",
       " 'NM_AUX_SERVICE_spark_shuffle': '',\n",
       " 'NM_HOST': 'ip-172-31-31-238.ec2.internal',\n",
       " 'NM_HTTP_PORT': '8042',\n",
       " 'NM_PORT': '8041',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin',\n",
       " 'PIDFILE': '/var/run/hadoop-yarn/yarn-yarn-nodemanager.pid',\n",
       " 'PWD': '/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002',\n",
       " 'PYTHONHASHSEED': '0',\n",
       " 'PYTHONPATH': '/mnt1/yarn/usercache/hadoop/filecache/44/__spark_libs__8229243635817091777.zip/spark-core_2.11-2.1.0.jar:geopyspark-and-friends.tar.gz/:/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip:/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002/pyspark.zip:/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/container_1490714960966_0007_01_000002/py4j-0.10.4-src.zip',\n",
       " 'PYTHONUNBUFFERED': 'YES',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '5',\n",
       " 'SLEEP_TIME': '10',\n",
       " 'SPARK_LOCAL_DIRS': '/mnt/yarn/usercache/hadoop/appcache/application_1490714960966_0007/blockmgr-4c507b67-ce69-4c89-874d-104721dd7829,/mnt1/yarn/usercache/hadoop/appcache/application_1490714960966_0007/blockmgr-bdbd0a4b-2d67-42cc-a930-b5de4c0b4684',\n",
       " 'SPARK_LOG_URL_STDERR': 'http://ip-172-31-31-238.ec2.internal:8042/node/containerlogs/container_1490714960966_0007_01_000002/hadoop/stderr?start=-4096',\n",
       " 'SPARK_LOG_URL_STDOUT': 'http://ip-172-31-31-238.ec2.internal:8042/node/containerlogs/container_1490714960966_0007_01_000002/hadoop/stdout?start=-4096',\n",
       " 'SPARK_REUSE_WORKER': '1',\n",
       " 'SPARK_USER': 'hadoop',\n",
       " 'SPARK_YARN_MODE': 'true',\n",
       " 'SPARK_YARN_STAGING_DIR': 'hdfs://ip-172-31-26-112.ec2.internal:8020/user/hadoop/.sparkStaging/application_1490714960966_0007',\n",
       " 'SVC_USER': 'yarn',\n",
       " 'TERM': 'linux',\n",
       " 'UPSTART_INSTANCE': '',\n",
       " 'UPSTART_JOB': 'hadoop-yarn-nodemanager',\n",
       " 'USER': 'hadoop',\n",
       " 'WORKING_DIR': '/var/lib/hadoop-yarn',\n",
       " 'XFILESEARCHPATH': '/usr/dt/app-defaults/%L/Dt',\n",
       " 'YARN_CONF_DIR': '/etc/hadoop/conf',\n",
       " 'YARN_IDENT_STRING': 'yarn',\n",
       " 'YARN_LOGFILE': 'yarn-yarn-nodemanager-ip-172-31-31-238.log',\n",
       " 'YARN_LOG_DIR': '/var/log/hadoop-yarn',\n",
       " 'YARN_NICENESS': '0',\n",
       " 'YARN_NODEMANAGER_HEAPSIZE': '2048',\n",
       " 'YARN_OPTS': \" -XX:OnOutOfMemoryError='kill -9 %p' -XX:OnOutOfMemoryError='kill -9 %p' -server  -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-nodemanager-ip-172-31-31-238.log -Dyarn.log.file=yarn-yarn-nodemanager-ip-172-31-31-238.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dhadoop.home.dir=/usr/lib/hadoop -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA -Dsun.net.inetaddr.ttl=30 -Djava.library.path=:/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/usr/lib/hadoop/lib/native\",\n",
       " 'YARN_PID_DIR': '/var/run/hadoop-yarn',\n",
       " 'YARN_PROXYSERVER_HEAPSIZE': '2396',\n",
       " 'YARN_RESOURCEMANAGER_HEAPSIZE': '2396',\n",
       " 'YARN_ROOT_LOGGER': 'INFO,DRFA',\n",
       " '_': '/usr/lib/jvm/java-openjdk/bin/java'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1]).map(moop1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xmax': 1.0, 'xmin': 0.0, 'ymax': 1.0, 'ymin': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (YARN)",
   "language": "python",
   "name": "pysparkyarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
